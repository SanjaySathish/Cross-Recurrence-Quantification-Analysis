{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdf5685-8b3d-4c29-859c-3b832f9c8f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN, LSTM\n",
    "from keras.layers import Dropout\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error,mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0708d8c-b9ce-4b89-bee0-aea0e56dcd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file = pd.read_csv(\"train_data2.csv\")\n",
    "print(train_data_file.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b759ae-a1c8-408f-a896-a29b6affe10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_price = train_data_file['\"HDFCBANK_M&M\"']\n",
    "plt.plot(train_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da2e139-8dc4-49dd-a706-18b96b508efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file['\"HDFCBANK_M&M\"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153c3a4b-2fe1-4cbc-bd2d-8663a444a455",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_file = pd.read_csv(\"test_data2.csv\")\n",
    "print(test_data_file.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627a5948-b15f-4004-b1a9-64d9ade99739",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_price = test_data_file['\"RELIANCE_UPL\"']\n",
    "plt.figure(figsize = (30,10))\n",
    "plt.plot(test_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c7a3c0-13af-47dd-97d9-9fff93011f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = train_data_file.columns\n",
    "names = list(names)\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d54af3-6fde-47c0-95b6-92330fbafc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"A\":[1,2,3],\"b\":[3,4,5],\"c\":[4,5,6]}\n",
    "df = pd.DataFrame(data, columns = [\"A\",\"b\",\"c\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f00f886-70c4-40d9-9ea5-b897169655e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"sample_out.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbbd8eb-63d5-4e1c-9ae6-f24ea79588a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e938e669-fddc-4901-ad23-7d49cb2f10f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_series_rnn = {}\n",
    "predicted_series_lstm = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4e8f4f-e04b-4348-a4de-088ae0ec4161",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "count = 0\n",
    "for file in names:\n",
    "    print(\"\\n***************************************************************************************************************\")\n",
    "    print(\"Pair Number:\", count)\n",
    "    count+=1\n",
    "    train_data = train_data_file[file]\n",
    "    test_data = test_data_file[file]\n",
    "    length_validation = len(test_data)\n",
    "    length_train = len(train_data)\n",
    "\n",
    "    train = np.array(train_data)\n",
    "    dataset_train = train\n",
    "    dataset_train = train.reshape(-1,1)\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range = (0,1))\n",
    "    \n",
    "    # scaling dataset\n",
    "    dataset_train_scaled = scaler.fit_transform(dataset_train)\n",
    "    \n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    time_step = 50\n",
    "    \n",
    "    for i in range(time_step, length_train):\n",
    "        X_train.append(dataset_train_scaled[i-time_step:i,0])\n",
    "        y_train.append(dataset_train_scaled[i,0])\n",
    "        \n",
    "    # convert list to array\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "    \n",
    "    \n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))\n",
    "    y_train = np.reshape(y_train, (y_train.shape[0],1))\n",
    "    \n",
    "    \n",
    "    # initializing the RNN\n",
    "    regressor = Sequential()\n",
    "    \n",
    "    # adding first RNN layer and dropout regulatization\n",
    "    regressor.add(SimpleRNN(units = 50, activation = \"tanh\", return_sequences = True, input_shape = (X_train.shape[1],1)))\n",
    "    \n",
    "    regressor.add(Dropout(0.2))\n",
    "    \n",
    "    # # adding third RNN layer and dropout regulatization\n",
    "    \n",
    "    regressor.add(SimpleRNN(units = 50, activation = \"tanh\", return_sequences = True))\n",
    "    \n",
    "    regressor.add(Dropout(0.2))\n",
    "    \n",
    "    # adding fourth RNN layer and dropout regulatization\n",
    "    \n",
    "    regressor.add(SimpleRNN(units = 50))\n",
    "    \n",
    "    regressor.add(Dropout(0.2))\n",
    "    \n",
    "    # adding the output layer\n",
    "    regressor.add(Dense(units = 1))\n",
    "    \n",
    "    # # compiling RNN\n",
    "    regressor.compile(\n",
    "        optimizer = \"adam\", \n",
    "        loss = \"mean_squared_error\")\n",
    "    \n",
    "    # fitting the RNN\n",
    "    history = regressor.fit(X_train, y_train, epochs = 35, batch_size = 32, shuffle = False)\n",
    "\n",
    "    y_train = scaler.inverse_transform(y_train) # scaling back from 0-1 to original\n",
    "    y_pred = regressor.predict(X_train)  # predictions\n",
    "    y_pred = scaler.inverse_transform(y_pred) # scaling back from 0-1 to original\n",
    "\n",
    "    \n",
    "    validation_data = np.array(test_data)\n",
    "    dataset_validation = validation_data  # getting \"open\" column and converting to array\n",
    "    dataset_validation = np.reshape(dataset_validation, (-1,1))  # converting 1D to 2D array\n",
    "    scaled_dataset_validation =  scaler.fit_transform(dataset_validation)  # scaling open values to between 0 and 1\n",
    "    \n",
    "    # Creating X_test and y_test\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    for i in range(time_step, length_validation):\n",
    "        X_test.append(scaled_dataset_validation[i-time_step:i,0])\n",
    "        y_test.append(scaled_dataset_validation[i,0])\n",
    "    \n",
    "    # Converting to array\n",
    "    X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "    \n",
    "    X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))  # reshape to 3D array\n",
    "    y_test = np.reshape(y_test, (-1,1))  # reshape to 2D array\n",
    "     \n",
    "    # predictions with X_test data\n",
    "    y_pred_of_test = regressor.predict(X_test)\n",
    "    # scaling back from 0-1 to original\n",
    "    y_pred_of_test = scaler.inverse_transform(y_pred_of_test) \n",
    "    flattened_results = np.array(y_pred_of_test).flatten().tolist()\n",
    "    predicted_series_rnn[names.index(file)] = flattened_results\n",
    "    # visualisation\n",
    "    plt.figure(figsize = (30,10))\n",
    "    plt.plot(y_pred_of_test, label = \"y_pred_of_test\", c = \"orange\")\n",
    "    plt.plot(scaler.inverse_transform(y_test) , label = \"y_test\", c = \"g\")\n",
    "    plt.xlabel(\"time\")\n",
    "    plt.ylabel(\"distance\")\n",
    "    plt.title(\"Testing Graph\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    R2score = r2_score(scaler.inverse_transform(y_test), y_pred_of_test)\n",
    "    rsme = np.sqrt(mean_squared_error(scaler.inverse_transform(y_test), y_pred_of_test))\n",
    "    mape = mean_absolute_percentage_error(scaler.inverse_transform(y_test), y_pred_of_test)\n",
    "    mae = mean_absolute_error(scaler.inverse_transform(y_test), y_pred_of_test)\n",
    "    print(\"\\nPrediction results:\")\n",
    "    results_rnn = [R2score, rsme, mae, mape]\n",
    "    y = scaler.inverse_transform(y_test)\n",
    "    # thresholds = [epsilon1[names.index(file)],epsilon2[names.index(file)],epsilon3[names.index(file)],epsilon4[names.index(file)]]\n",
    "    \n",
    "    print(\"r2_score: \", R2score)\n",
    "    print(\"MAPE: \", mape)\n",
    "    print(\"MAE: \", mae)\n",
    "    print(\"RMSE: \", rsme)\n",
    "   \n",
    "    print(\"_____________________________\")\n",
    "    \n",
    "    \n",
    "    ###LSTM#####\n",
    "    \n",
    "    train_data = train_data_file[file]\n",
    "    test_data = test_data_file[file]\n",
    "    length_validation = len(test_data)\n",
    "    length_train = len(train_data)\n",
    "    \n",
    "    train = np.array(train_data)\n",
    "    dataset_train = train\n",
    "    dataset_train = train.reshape(-1,1)\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range = (0,1))\n",
    "    \n",
    "    # scaling dataset\n",
    "    dataset_train_scaled = scaler.fit_transform(dataset_train)\n",
    "    \n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    time_step = 50\n",
    "    \n",
    "    for i in range(time_step, length_train):\n",
    "        X_train.append(dataset_train_scaled[i-time_step:i,0])\n",
    "        y_train.append(dataset_train_scaled[i,0])\n",
    "        \n",
    "    # convert list to array\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "    \n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))\n",
    "    y_train = np.reshape(y_train, (y_train.shape[0],1))\n",
    "    \n",
    "    #Initialise LSTM\n",
    "    regressor = Sequential()\n",
    "    \n",
    "    regressor.add(LSTM(units = 50, activation = \"tanh\", return_sequences = True, input_shape = (X_train.shape[1],1)))\n",
    "    regressor.add(Dropout(0.2))\n",
    "    \n",
    "    regressor.add(LSTM(units = 50, activation = \"tanh\"))\n",
    "    regressor.add(Dropout(0.2))\n",
    "    \n",
    "    regressor.add(Dense(units = 1))\n",
    "    \n",
    "    # # compiling RNN\n",
    "    regressor.compile(\n",
    "        optimizer = \"adam\", \n",
    "        loss = \"mean_squared_error\")\n",
    "    \n",
    "    # fitting the RNN\n",
    "    history = regressor.fit(X_train, y_train, epochs = 35, batch_size = 32, shuffle = False)\n",
    "    \n",
    "    y_train = scaler.inverse_transform(y_train) # scaling back from 0-1 to original\n",
    "    y_pred = regressor.predict(X_train)  # predictions\n",
    "    y_pred = scaler.inverse_transform(y_pred) # scaling back from 0-1 to original\n",
    "    \n",
    "    validation_data = np.array(test_data)\n",
    "    dataset_validation = validation_data  # getting \"open\" column and converting to array\n",
    "    dataset_validation = np.reshape(dataset_validation, (-1,1))  # converting 1D to 2D array\n",
    "    scaled_dataset_validation =  scaler.fit_transform(dataset_validation)  # scaling open values to between 0 and 1\n",
    "    \n",
    "    # Creating X_test and y_test\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    for i in range(time_step, length_validation):\n",
    "        X_test.append(scaled_dataset_validation[i-time_step:i,0])\n",
    "        y_test.append(scaled_dataset_validation[i,0])\n",
    "    \n",
    "    # Converting to array\n",
    "    X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "    \n",
    "    X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))  # reshape to 3D array\n",
    "    y_test = np.reshape(y_test, (-1,1))  # reshape to 2D array\n",
    "    \n",
    "    \n",
    "    # predictions with X_test data\n",
    "    y_pred_of_test = regressor.predict(X_test)\n",
    "    # scaling back from 0-1 to original\n",
    "    y_pred_of_test = scaler.inverse_transform(y_pred_of_test) \n",
    "    flattened_results = np.array(y_pred_of_test).flatten().tolist()\n",
    "    predicted_series_lstm[names.index(file)] = flattened_results\n",
    "    # visualisation\n",
    "    plt.figure(figsize = (30,10))\n",
    "    plt.plot(y_pred_of_test, label = \"y_pred_of_test\", c = \"orange\")\n",
    "    plt.plot(scaler.inverse_transform(y_test), label = \"y_test\", c = \"green\")\n",
    "    plt.xlabel(\"time\")\n",
    "    plt.ylabel(\"distance\")\n",
    "    plt.title(\"Testing Graph\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    R2score = r2_score(scaler.inverse_transform(y_test), y_pred_of_test)\n",
    "    rsme = np.sqrt(mean_squared_error(scaler.inverse_transform(y_test), y_pred_of_test))\n",
    "    mape = mean_absolute_percentage_error(scaler.inverse_transform(y_test), y_pred_of_test)\n",
    "    mae = mean_absolute_error(scaler.inverse_transform(y_test), y_pred_of_test)\n",
    "    print(\"\\nPrediction results:\")\n",
    "    \n",
    "    results_lstm = [R2score, rsme, mae, mape]\n",
    "    y = scaler.inverse_transform(y_test)\n",
    "    # thresholds = [epsilon1[names.index(file)],epsilon2[names.index(file)],epsilon3[names.index(file)],epsilon4[names.index(file)]]\n",
    "    \n",
    "    \n",
    "    print(\"r2_score: \", R2score)\n",
    "    print(\"MAPE: \", mape)\n",
    "    print(\"MAE: \", mae)\n",
    "    print(\"RSME: \", rsme)\n",
    "\n",
    "    final_results = [file]\n",
    "    final_results.append(results_rnn)\n",
    "    final_results.append(results_lstm)\n",
    "    print(final_results)\n",
    "    rows.append(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da713098-5d46-4e49-b7a4-0d0901cdb5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(predicted_series_rnn)\n",
    "df.columns = names\n",
    "df.to_csv(\"predicted_series_rnn_block10_w50.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17590b07-595f-4b2d-aa2d-b574672f48a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(predicted_series_lstm)\n",
    "df.columns = names\n",
    "df.to_csv(\"predicted_series_lstm_block10_w50.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fb40ed-55eb-432d-8fcb-166423e60cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_r2 = []\n",
    "rnn_rmse = []\n",
    "rnn_mae = []\n",
    "rnn_mape = []\n",
    "lstm_r2 = []\n",
    "lstm_rmse = []\n",
    "lstm_mae = []\n",
    "lstm_mape = []\n",
    "\n",
    "results_rows = []\n",
    "for i in range (len(rows)):\n",
    "    row = rows[i]\n",
    "    rnn = row[1]\n",
    "    rnn_r2.append(rnn[0])\n",
    "    rnn_rmse.append(rnn[1])\n",
    "    rnn_mae.append(rnn[2])\n",
    "    rnn_mape.append(rnn[3])\n",
    "    \n",
    "    lstm = row[2]\n",
    "    \n",
    "    lstm_r2.append(lstm[0])\n",
    "    lstm_rmse.append(lstm[1])\n",
    "    lstm_mae.append(lstm[2])\n",
    "    lstm_mape.append(lstm[3])\n",
    "    \n",
    "results_rows = [rnn_r2,rnn_rmse,rnn_mae,\n",
    "                 \n",
    "                lstm_r2,lstm_rmse,lstm_mae,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4861f2-2234-497e-8128-470fe4e1ca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_acc = []\n",
    "good = []\n",
    "mid = []\n",
    "bad = []\n",
    "for i in range(len(rnn_mape)):\n",
    "    if lstm_mape[i]<=0.10:\n",
    "        high_acc.append(lstm_mape[i])\n",
    "    elif lstm_mape[i]<=0.20 and lstm_mape[i]>0.10:\n",
    "        good.append(lstm_mape[i])\n",
    "    elif lstm_mape[i]<=0.50 and lstm_mape[i]>0.20:\n",
    "        mid.append(lstm_mape[i])\n",
    "    elif lstm_mape[i]>0.50:\n",
    "        bad.append(lstm_mape[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdca9c9-0f04-4750-8bd0-055757f274a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(rnn_mape))\n",
    "print(len(lstm_mape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acacc350-8521-4530-8a99-315c866359de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean \n",
    "\n",
    "print(\"Highly accuracte MAPE count: \", len(high_acc), \"-> avg = \",mean(high_acc))\n",
    "print(\"Good accuracte MAPE count: \", len(good),\"-> avg = \",mean(good))\n",
    "print(\"Reasonably good accuracte MAPE count: \", len(mid), \"-> avg = \",mean(mid))\n",
    "print(\"Inaccuracte MAPE count: \", len(bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caed8bfd-a503-42a4-8849-92190e5713fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_acc = []\n",
    "good = []\n",
    "mid = []\n",
    "bad = []\n",
    "for i in range(len(rnn_mape)):\n",
    "    if rnn_mape[i]<=0.10:\n",
    "        high_acc.append(rnn_mape[i])\n",
    "    elif rnn_mape[i]<=0.20 and rnn_mape[i]>0.10:\n",
    "        good.append(rnn_mape[i])\n",
    "    elif rnn_mape[i]<=0.50 and rnn_mape[i]>0.20:\n",
    "        mid.append(rnn_mape[i])\n",
    "    elif rnn_mape[i]>0.50:\n",
    "        bad.append(rnn_mape[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7ad8bf-072e-4b6a-a3d1-5ff3c3f8f115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean \n",
    "\n",
    "print(\"Highly accuracte MAPE count: \", len(high_acc), \"-> avg = \",mean(high_acc))\n",
    "print(\"Good accuracte MAPE count: \", len(good),\"-> avg = \",mean(good))\n",
    "print(\"Reasonably good accuracte MAPE count: \", len(mid), \"-> avg = \",mean(mid))\n",
    "print(\"Inaccuracte MAPE count: \", len(bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad67cd46-6863-4d0c-a751-29e271fd1eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['Name', 'RNN results', 'LSTM results']\n",
    "filename = 'PriceVolBlock_Results_block10_w50.csv'\n",
    "#RNN, LSTM Results look like: [[r2score, rsme, mae, accuracy1, f1_1, f1_indiv[0]_1,f1_indiv[1]_1, y_a.count(0), y_a.count(1)],..]\n",
    "with open(filename, 'w') as csvfile:\n",
    "    # creating a csv writer object\n",
    "    csvwriter = csv.writer(csvfile)\n",
    " \n",
    "    # writing the fields\n",
    "    csvwriter.writerow(fields)\n",
    " \n",
    "    # writing the data rows\n",
    "    csvwriter.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462f88ff-8ed0-48a8-b024-07eb9caf3c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf65f69-4ea9-41ac-8da0-446c3e578cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_r2 = []\n",
    "rnn_rmse = []\n",
    "rnn_mae = []\n",
    "rnn_mape = []\n",
    "\n",
    "rnn_f1_1_e1 = []\n",
    "rnn_f1_e1 = []\n",
    "rnn_acc_e1 = []\n",
    "rnn_per_e1 = []\n",
    "\n",
    "rnn_f1_1_e2 = []\n",
    "rnn_f1_e2 = []\n",
    "rnn_acc_e2 = []\n",
    "rnn_per_e2 = []\n",
    "\n",
    "rnn_f1_1_e3 = []\n",
    "rnn_f1_e3 = []\n",
    "rnn_acc_e3 = []\n",
    "rnn_per_e3 = []\n",
    "\n",
    "rnn_f1_1_e4 = []\n",
    "rnn_f1_e4 = []\n",
    "rnn_acc_e4 = []\n",
    "rnn_per_e4 = []\n",
    "\n",
    "lstm_r2 = []\n",
    "lstm_rmse = []\n",
    "lstm_mae = []\n",
    "lstm_mape = []\n",
    "\n",
    "lstm_f1_1_e1 = []\n",
    "lstm_f1_e1 = []\n",
    "lstm_acc_e1 = []\n",
    "lstm_per_e1 = []\n",
    "\n",
    "lstm_f1_1_e2 = []\n",
    "lstm_f1_e2 = []\n",
    "lstm_acc_e2 = []\n",
    "lstm_per_e2 = []\n",
    "\n",
    "lstm_f1_1_e3 = []\n",
    "lstm_f1_e3 = []\n",
    "lstm_acc_e3 = []\n",
    "lstm_per_e3 = []\n",
    "\n",
    "lstm_f1_1_e4 = []\n",
    "lstm_f1_e4 = []\n",
    "lstm_acc_e4 = []\n",
    "lstm_per_e4 = []\n",
    "\n",
    "results_rows = []\n",
    "for i in range (len(rows)):\n",
    "    row = rows[i]\n",
    "    rnn = row[1]\n",
    "    rnn_r2.append(rnn[0])\n",
    "    rnn_rmse.append(rnn[1])\n",
    "    rnn_mae.append(rnn[2])\n",
    "    rnn_mape.append(rnn[3])\n",
    "    \n",
    "    rnn_acc_e1.append(rnn[4][0])\n",
    "    rnn_f1_e1.append(rnn[4][1])\n",
    "    rnn_f1_1_e1.append(rnn[4][2])\n",
    "    rnn_per_e1.append(rnn[4][3])\n",
    "\n",
    "    rnn_acc_e2.append(rnn[5][0])\n",
    "    rnn_f1_e2.append(rnn[5][1])\n",
    "    rnn_f1_1_e2.append(rnn[5][2])\n",
    "    rnn_per_e2.append(rnn[5][3])\n",
    "\n",
    "    rnn_acc_e3.append(rnn[6][0])\n",
    "    rnn_f1_e3.append(rnn[6][1])\n",
    "    rnn_f1_1_e3.append(rnn[6][2])\n",
    "    rnn_per_e3.append(rnn[6][3])\n",
    "\n",
    "    rnn_acc_e4.append(rnn[7][0])\n",
    "    rnn_f1_e4.append(rnn[7][1])\n",
    "    rnn_f1_1_e4.append(rnn[7][2])\n",
    "    rnn_per_e4.append(rnn[7][3])\n",
    "    \n",
    "    lstm = row[2]\n",
    "    \n",
    "    lstm_r2.append(lstm[0])\n",
    "    lstm_rmse.append(lstm[1])\n",
    "    lstm_mae.append(lstm[2])\n",
    "    lstm_mape.append(lstm[3])\n",
    "    \n",
    "    lstm_acc_e1.append(lstm[4][0])\n",
    "    lstm_f1_e1.append(lstm[4][1])\n",
    "    lstm_f1_1_e1.append(lstm[4][2])\n",
    "    lstm_per_e1.append(lstm[4][3])\n",
    "\n",
    "    lstm_acc_e2.append(lstm[5][0])\n",
    "    lstm_f1_e2.append(lstm[5][1])\n",
    "    lstm_f1_1_e2.append(lstm[5][2])\n",
    "    lstm_per_e2.append(lstm[5][3])\n",
    "\n",
    "    lstm_acc_e3.append(lstm[6][0])\n",
    "    lstm_f1_e3.append(lstm[6][1])\n",
    "    lstm_f1_1_e3.append(lstm[6][2])\n",
    "    lstm_per_e3.append(lstm[6][3])\n",
    "    \n",
    "    lstm_acc_e4.append(lstm[7][0])\n",
    "    lstm_f1_e4.append(lstm[7][1])\n",
    "    lstm_f1_1_e4.append(lstm[7][2])\n",
    "    lstm_per_e4.append(lstm[7][3])\n",
    "    \n",
    "results_rows = [rnn_r2,rnn_rmse,rnn_mae,\n",
    "                rnn_f1_1_e1,rnn_f1_e1,rnn_acc_e1,rnn_per_e1,\n",
    "                rnn_f1_1_e2,rnn_f1_e2,rnn_acc_e2,rnn_per_e2,\n",
    "                rnn_f1_1_e3,rnn_f1_e3,rnn_acc_e3,rnn_per_e3,\n",
    "                rnn_f1_1_e4,rnn_f1_e4,rnn_acc_e4,rnn_per_e4,\n",
    "                lstm_r2,lstm_rmse,lstm_mae,\n",
    "                lstm_f1_1_e1,lstm_f1_e1,lstm_acc_e1,lstm_per_e1,\n",
    "                lstm_f1_1_e2,lstm_f1_e2,lstm_acc_e2,lstm_per_e2,\n",
    "                lstm_f1_1_e3,lstm_f1_e3,lstm_acc_e3,lstm_per_e3,\n",
    "               lstm_f1_1_e4,lstm_f1_e4,lstm_acc_e4,lstm_per_e4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd8022-2e62-459d-934a-9526e78e04be",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'ResultsRows1.csv'\n",
    "with open(filename, 'w') as csvfile:\n",
    "    # creating a csv writer object\n",
    "    csvwriter = csv.writer(csvfile)\n",
    " \n",
    "    # writing the data rows\n",
    "    csvwriter.writerows(results_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3134c75e-f342-4972-b041-07afe3939dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_acc = []\n",
    "good = []\n",
    "mid = []\n",
    "bad = []\n",
    "for i in range(len(rnn_mape)):\n",
    "    if lstm_mape[i]<=0.10:\n",
    "        high_acc.append(lstm_mape[i])\n",
    "    elif lstm_mape[i]<=0.20 and lstm_mape[i]>0.10:\n",
    "        good.append(lstm_mape[i])\n",
    "    elif lstm_mape[i]<=0.50 and lstm_mape[i]>0.20:\n",
    "        mid.append(lstm_mape[i])\n",
    "    elif lstm_mape[i]>0.50:\n",
    "        bad.append(lstm_mape[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca88f7d-d568-40f5-9313-0fdba01eb99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Highly accuracte MAPE count: \", len(high_acc), \"-> avg = \",mean(high_acc))\n",
    "print(\"Good accuracte MAPE count: \", len(good),\"-> avg = \",mean(good))\n",
    "print(\"Reasonably good accuracte MAPE count: \", len(mid), \"-> avg = \",mean(mid))\n",
    "print(\"Inaccuracte MAPE count: \", len(bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e3211c-a89e-4286-ad8c-b560e5f7faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(rnn_mape)):\n",
    "    if lstm_mape[i]>0.50:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad5ddd-4c44-4efe-b1a8-aeb94b8441f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_acc = []\n",
    "good = []\n",
    "mid = []\n",
    "bad = []\n",
    "for i in range(len(rnn_mape)):\n",
    "    if rnn_mape[i]<=0.10:\n",
    "        high_acc.append(rnn_mape[i])\n",
    "    elif rnn_mape[i]<=0.20 and rnn_mape[i]>0.10:\n",
    "        good.append(lstm_mape[i])\n",
    "    elif rnn_mape[i]<=0.50 and rnn_mape[i]>0.20:\n",
    "        mid.append(rnn_mape[i])\n",
    "    elif rnn_mape[i]>0.50:\n",
    "        bad.append(rnn_mape[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ac8f90-fd54-41a3-aebf-de7a3075d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Highly accuracte MAPE count: \", len(high_acc), \"-> avg = \",mean(high_acc))\n",
    "print(\"Good accuracte MAPE count: \", len(good),\"-> avg = \",mean(good))\n",
    "print(\"Reasonably good accuracte MAPE count: \", len(mid), \"-> avg = \",mean(mid))\n",
    "print(\"Inaccuracte MAPE count: \", len(bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d46226-f290-480b-96f6-f85085980cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = ['HDFCBANK','MARUTI', 'M&M', 'ULTRACEMCO', 'GRASIM', 'PIDILITIND', 'LT', 'DLF','HINDUNILVR','ITC','RELIANCE','ONGC','UPL','ICICIBANK','SIEMENS',\n",
    "          'ABB','TCS','INFY','COALINDIA','TATASTEEL','SUNPHARMA','DIVISLAB','ADANIPORTS','CONCOR','BHARTIARTL','IDEA']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
